{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed5e613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cpu\n",
      "0.19.1+cpu\n",
      "privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch_directml\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "if torch_directml.is_available():\n",
    "    device = torch_directml.device()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vs_test_loss(train_losses, test_losses, epochs):\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Test Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80c975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 0.7\n",
    "b = 0.3\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "x = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = w * x + b\n",
    "x[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ffd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.8 * len(x))\n",
    "x_train, y_train = x[:train_split], y[:train_split]\n",
    "x_test, y_test = x[train_split:], y[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612793f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8000],\n",
       "         [0.8200],\n",
       "         [0.8400],\n",
       "         [0.8600],\n",
       "         [0.8800],\n",
       "         [0.9000],\n",
       "         [0.9200],\n",
       "         [0.9400],\n",
       "         [0.9600],\n",
       "         [0.9800]]),\n",
       " tensor([[0.8600],\n",
       "         [0.8740],\n",
       "         [0.8880],\n",
       "         [0.9020],\n",
       "         [0.9160],\n",
       "         [0.9300],\n",
       "         [0.9440],\n",
       "         [0.9580],\n",
       "         [0.9720],\n",
       "         [0.9860]]),\n",
       " tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800],\n",
       "         [0.2000],\n",
       "         [0.2200],\n",
       "         [0.2400],\n",
       "         [0.2600],\n",
       "         [0.2800],\n",
       "         [0.3000],\n",
       "         [0.3200],\n",
       "         [0.3400],\n",
       "         [0.3600],\n",
       "         [0.3800],\n",
       "         [0.4000],\n",
       "         [0.4200],\n",
       "         [0.4400],\n",
       "         [0.4600],\n",
       "         [0.4800],\n",
       "         [0.5000],\n",
       "         [0.5200],\n",
       "         [0.5400],\n",
       "         [0.5600],\n",
       "         [0.5800],\n",
       "         [0.6000],\n",
       "         [0.6200],\n",
       "         [0.6400],\n",
       "         [0.6600],\n",
       "         [0.6800],\n",
       "         [0.7000],\n",
       "         [0.7200],\n",
       "         [0.7400],\n",
       "         [0.7600],\n",
       "         [0.7800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260],\n",
       "         [0.4400],\n",
       "         [0.4540],\n",
       "         [0.4680],\n",
       "         [0.4820],\n",
       "         [0.4960],\n",
       "         [0.5100],\n",
       "         [0.5240],\n",
       "         [0.5380],\n",
       "         [0.5520],\n",
       "         [0.5660],\n",
       "         [0.5800],\n",
       "         [0.5940],\n",
       "         [0.6080],\n",
       "         [0.6220],\n",
       "         [0.6360],\n",
       "         [0.6500],\n",
       "         [0.6640],\n",
       "         [0.6780],\n",
       "         [0.6920],\n",
       "         [0.7060],\n",
       "         [0.7200],\n",
       "         [0.7340],\n",
       "         [0.7480],\n",
       "         [0.7620],\n",
       "         [0.7760],\n",
       "         [0.7900],\n",
       "         [0.8040],\n",
       "         [0.8180],\n",
       "         [0.8320],\n",
       "         [0.8460]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test, y_test, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=x_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=x_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "    plt.legend(prop={\"size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07472ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.w * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f08971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3982],\n",
       "         [0.4049],\n",
       "         [0.4116],\n",
       "         [0.4184],\n",
       "         [0.4251],\n",
       "         [0.4318],\n",
       "         [0.4386],\n",
       "         [0.4453],\n",
       "         [0.4520],\n",
       "         [0.4588]]),\n",
       " tensor([[0.8600],\n",
       "         [0.8740],\n",
       "         [0.8880],\n",
       "         [0.9020],\n",
       "         [0.9160],\n",
       "         [0.9300],\n",
       "         [0.9440],\n",
       "         [0.9580],\n",
       "         [0.9720],\n",
       "         [0.9860]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(x_test)\n",
    "\n",
    "y_preds, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753edb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions= y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224e1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  10 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  20 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  30 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  40 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  50 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  60 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  70 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  80 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch:  90 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 1990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 2990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 3990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 4990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 5990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 6990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 7990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 8990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 9990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 10990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 11990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 12990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 13990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 14990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 15990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 16990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 17990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 18990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 19990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 20990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 21990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 22990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 23990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 24990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 25990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 26990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 27990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 28990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 29990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 30990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 31990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 32990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 33990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 34990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 35990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 36990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 37990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 38990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 39990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 40990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 41990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 42990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 43990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 44990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 45990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 46990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 47990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 48990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 49990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 50990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 51990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 52990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 53990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 54990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 55990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 56990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 57990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 58990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 59990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 60990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 61990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 62990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 63990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 64990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 65990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 66990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 67990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 68990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 69990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 70990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 71990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 72990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 73990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 74990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 75990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 76990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 77990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 78990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 79990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 80990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 81990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 82990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 83990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 84990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 85990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 86990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 87990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 88990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 89990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 90990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 91990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 92990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 93990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 94990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 95990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 96990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 97990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 98990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99000 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99010 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99020 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99030 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99040 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99050 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99060 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99070 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99080 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99090 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99100 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99110 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99120 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99130 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99140 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99150 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99160 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99170 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99180 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99190 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99200 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99210 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99220 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99230 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99240 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99250 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99260 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99270 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99280 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99290 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99300 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99310 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99320 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99330 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99340 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99350 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99360 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99370 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99380 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99390 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99400 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99410 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99420 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99430 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99440 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99450 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99460 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99470 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99480 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99490 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99500 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99510 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99520 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99530 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99540 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99550 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99560 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99570 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99580 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99590 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99600 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99610 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99620 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99630 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99640 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99650 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99660 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99670 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99680 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99690 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99700 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99710 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99720 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99730 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99740 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99750 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99760 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99770 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99780 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99790 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99800 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99810 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99820 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99830 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99840 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99850 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99860 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99870 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99880 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99890 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99900 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99910 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99920 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99930 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99940 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99950 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99960 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99970 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99980 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Epoch: 99990 | Train Loss: 0.0026 | Test Loss: 0.0084 | W: 0.6990 | B: 0.3093\n",
      "Test loss: 0.005023092031478882\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Testing (every 10 epochs)\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = loss_fn(y_test_pred, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "            \n",
    "        print(f\"Epoch: {epoch:3d} | Train Loss: {loss:.4f} | Test Loss: {test_loss:.4f} | W: {model.w.item():.4f} | B: {model.b.item():.4f}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(x_test)\n",
    "    test_loss = loss_fn(y_preds, y_test)\n",
    "    print(f\"Test loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecfc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss: 0.0050\n",
      "Target: W=0.7, B=0.3\n",
      "Learned: W=0.6951, B=0.2993\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(x_test)\n",
    "    final_test_loss = loss_fn(y_preds, y_test)\n",
    "    print(f\"\\nFinal Test Loss: {final_test_loss:.4f}\")\n",
    "    print(f\"Target: W={w}, B={b}\")\n",
    "    print(f\"Learned: W={model.w.item():.4f}, B={model.b.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b78d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_vs_test_loss(\u001b[43mtrain_losses\u001b[49m, test_losses, epochs=EPOCHS)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "train_vs_test_loss(train_losses, test_losses, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DL_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
